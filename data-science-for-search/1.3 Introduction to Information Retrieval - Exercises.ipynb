{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Introduction to Information Retrieval\n",
    "\n",
    "Here we work with a data set scraped from eBay.  The data contains 9895 item titles and descriptions.\n",
    "\n",
    "First we load the data file and _normalise_ the text - removing certain characters and converting to lower case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "with open(\"data/bike-items.txt\") as f:\n",
    "    r = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    rgx = re.compile(r'\\b[a-zA-Z]+\\b') \n",
    "    docs = [ (' '.join(re.findall(rgx, x[0])).lower(), ' '.join(re.findall(rgx, x[1])).lower())  \\\n",
    "                for i,x in enumerate(r) if i > 1 ]\n",
    "    \n",
    "print(docs[0][0],docs[0][1])\n",
    "\n",
    "items_t = [ d[0] for d in docs ] # item titles\n",
    "items_d = [ d[1] for d in docs ] # item descriptions\n",
    "items_i = range(0, len(items_t)) # item id\n",
    "#testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 1 - Term Frequency\n",
    "\n",
    "Let's start with the first 10 item titles from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = items_t[0:5]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by computing the frequency of terms in the *entire* corpus.  We will do this by enumerating over the corpus of documents, tokenizing the documents and count the frequency of tokens.   The easiest way is to build a python dictionary where the key is the token and the value is the count.  You can review python dictionaries in the [docs](https://docs.python.org/2/tutorial/datastructures.html).\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Here is a part completed code snippet to compute term frequency.  \n",
    "Complete this code to correctly populate the term frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = {}\n",
    "for doc in corpus:\n",
    "    for word in doc.split():\n",
    "        # << COMPUTE TERM FREQUENCY DICTIONARY >> CODE HERE\n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify by using a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) rather than a dictionary.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'> \n",
    "Take a look at the docs for the `Counter` collection.  \n",
    "Complete this function definition to compute term frequency using the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_tf(corpus):\n",
    "    tf = Counter()\n",
    "    for doc in corpus:\n",
    "        for word in doc.split():\n",
    "            # << CODE HERE\n",
    "            \n",
    "    return tf\n",
    "\n",
    "tf = get_tf(corpus)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Counter does not give us a real speed advantage - since it does more work.   For these tiny data sets we do not see any difference - however in Python 3 it is faster than a default dictionary.  Often times best way to test performance is to time code execution.\n",
    "\n",
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> \n",
    "We should get used to thinking about performance.   \n",
    "We can use the Jupyter Notebook [magics](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) to time the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Run the code to compute the term frequency for the full corpus of item titles.  \n",
    "What is the frequency of the terms 'unicycle', 'bicycle' and 'tricycle'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = get_tf(items_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency can also be computed for each document - the term frequency is a crude measure of the \"aboutness\" of a document.  For short documents, such as eBay item titles, terms do not occur very frequently.  In longer documents the term frequency is a form of compression and summarization.\n",
    "\n",
    "We can store the document term frequency in a dictionary, where the key is the document id and the value is the a nested dictionary of document terms and their counts.\n",
    "\n",
    "For example consider the corpus of three documents:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "The document term frequencies would be:\n",
    "\n",
    "| id | document term frequencies |\n",
    "|----|------------------|\n",
    "| 1  | { 'mountain' : 1, 'bike' : 1, 'red' : 1 } |\n",
    "| 2  | { 'road' : 1, 'bike' : 1, 'carbon' : 1 } |\n",
    "| 3  | { 'bike' : 1, 'helmet' : 1 } |\n",
    "\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Now compute **document term frequencies** for the full corpus of item titles.  \n",
    "Print out the document term frequencies for 3 randomly selected documents - what was the highest frequency term for each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfd(corpus):\n",
    "    tfd = {}\n",
    "    for i,doc in enumerate(corpus):\n",
    "        tfd[i]={}\n",
    "        # << DOCUMENT TERM FREQUENCY >> CODE HERE\n",
    "        \n",
    "    return tfd\n",
    "            \n",
    "    \n",
    "tfd = get_tfd(items_t)\n",
    "tfd[234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Repeat this time computing the **document term frequencies** for the full corpus of item descriptions.  \n",
    "Print out the document term frequency for 3 randomly selected documents - what was the highest frequency term for each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 2 - Term Frequency Ranking, Boolean Matching and Inverted Indexes\n",
    "\n",
    "Whilst the document term frequency dictionary in the previous section `tfd` is a compact way to store the term frequency it is not efficient for analysis.  A term frequency matrix is a more effective way to store the data.  \n",
    "\n",
    "For example consider the corpus of three documents:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "There is a toal vocabulary of six terms [ 'mountain', 'bike' , 'red', 'road', 'carbon', 'helmet' ].\n",
    "\n",
    "Each document count be represented as a 3 x 6 element vectors:\n",
    "\n",
    "| Document ID | mountain | bike | red | road | carbon | helmet |\n",
    "|-------------|----------|------|-----|------|--------|--------|\n",
    "| 1 - 'mountain bike red' | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| 2 - 'road bike carbon' | 0 | 1 | 0 | 1 | 1 | 0 |\n",
    "| 3 - 'bike helmet' | 0 | 1 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "Those arrays can be stacked naturally into a matrix - one row per document, one column per term.  We call this matrix the term frequency matrix.\n",
    "\n",
    "To compute the term frequency matrix we have to first compute the lexicon (set of terms) in our corpus.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Review the docs for the [set](https://docs.python.org/2/library/stdtypes.html#set-types-set-frozenset) type. Note - sets do not contain duplicates and can be used to dedupe tokens.\n",
    "Complete the `get_lexicon()` function definition so that it returns a list of unique terms across a given corpus of documents.  Validate with the small test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    # << COMPUTE SET OF TERMS IN CORPUS >> CODE HERE\n",
    "    \n",
    "    return list(lexicon)\n",
    "    \n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "lexicon = get_lexicon(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our lexicon we can compute a document term frequency matrix.  We will store our document term frequency vectors in a `list`.  Note we could also store them in a `dictionary` where the key is the document_id.  \n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Complete the code snippet below to compute the term frequency vector for each document.  \n",
    "Store the term frequency vectors in the list `tfm`.  Validate the results with the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = get_lexicon(test_corpus)\n",
    "\n",
    "tfm =[]\n",
    "for doc in test_corpus:\n",
    "    tfv = [0]*len(lexicon)\n",
    "    for term in doc.split():\n",
    "        # << COMPUTE DOCUMENT TERM FREQUENCY VECTOR tfv AND APPEND TO tfm >> CODE HERE\n",
    "        \n",
    "    tfm.append(tfv)\n",
    "    \n",
    "print(tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to reuse the tfm let's create a function that takes as argument the corpus and returns the lexicon and the tfm.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Copy your code snippets from the previous two exercises into the function definition below.    \n",
    "Test your function by computing 'tfm' on the test corpus verifying your results before computing tfm for the item_titles corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfm(corpus):\n",
    "    \n",
    "    def get_lexicon(corpus):\n",
    "        lexicon = set()\n",
    "        # << COMPUTE SET OF TERMS IN CORPUS >> CODE HERE\n",
    "        \n",
    "        \n",
    "    lexicon = get_lexicon(corpus)\n",
    "\n",
    "    tfm =[]\n",
    "    for doc in corpus:\n",
    "        tfv = [0]*len(lexicon)\n",
    "        for term in doc.split():\n",
    "            # << COMPUTE DOCUMENT TERM FREQUENCY VECTOR AND APPEND TO tfm >> CODE HERE\n",
    "            \n",
    "        tfm.append(tfv)\n",
    "        \n",
    "    return tfm, lexicon\n",
    "\n",
    "\n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "tfm, lexicon = get_tfm(test_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> \n",
    "As our corpus increases so does the sparsity of the term frequency matrix - most elements have value zero.  \n",
    "We can use more efficient [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) storage to save memory.  More details [here](http://localhost:8888/notebooks/1.3%20Introduction%20to%20Information%20Retrieval.ipynb#Sparse-Term-Frequency-Matrix).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.plotting import figure, output_notebook, show, vplot\n",
    "#from bokeh.charts import Bar, Scatter, BoxPlot\n",
    "#from bokeh.charts.attributes import CatAttr\n",
    "#from bokeh.models import ColumnDataSource\n",
    "\n",
    "# Sparsity as a function of document count\n",
    "n = []\n",
    "s = []\n",
    "for i in range(100,1000,100):\n",
    "    corpus = items_t[0:i]\n",
    "    tfm, lexicon = get_tfm(corpus)\n",
    "    c =[ [x.count(0), x.count(1)] for x in tfm]\n",
    "    n_zero = sum([ y[0] for y in c])\n",
    "    n_one = sum([ y[1] for y in c])  \n",
    "    s.append(1.0 - (float(n_one) / (n_one + n_zero)))\n",
    "    n.append(i)\n",
    "    \n",
    "output_notebook(hide_banner=True)\n",
    "p = figure(x_axis_label='Documents', y_axis_label='Sparsity',\n",
    "          plot_width=400, plot_height=400)\n",
    "p.line(n, s, line_width=2)\n",
    "p.circle(n, s, fill_color=\"white\", size=8)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Boolean Search\n",
    "\n",
    "We are now in a position to write our first ranking function.  Now we have the term frequency matrix we can use it to find documents that contain words included in a user specified query.  We will start by simply returning the documents from the corpus that match any terms in the query and rank by the raw frequency of matching terms. \n",
    "\n",
    "More specifically our algorithm for 'boolean search' proceeds as follows:\n",
    "\n",
    "1. Convert query to query vector using the lexicon for the corpus\n",
    "2. Compute a ranking score for each document by taking the [dot product](https://en.wikipedia.org/wiki/Dot_product) of the query vector and each document's term frequency vector\n",
    "3. Sort the documents by score\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "The function definion `get_results_tf()` converts the user query `qry` into a vector using the supplied lexicon.\n",
    "Complete the function by providing the code to compute the score of each document.  Test using a bike related query such as 'led bike light'.  Do you get relevant results?  \n",
    "\n",
    "HINT : Here is a [gist](https://gist.github.com/mattwg/60910d90a8987e271212) that shows how to compute the dot product between two vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_results_tf(qry, tfm, lexicon):\n",
    "    qrv = [0]*len(lexicon)\n",
    "    for term in qry.split():\n",
    "        if term in lexicon:\n",
    "            qrv[lexicon.index(term)] = 1\n",
    "\n",
    "    results = []      \n",
    "    for i, tfv in enumerate(tfm):\n",
    "        score = 0\n",
    "        # << COMPUTE DOCUMENT SCORE >> CODE HERE\n",
    "        \n",
    "        results.append([score, i])\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results\n",
    "\n",
    "\n",
    "def print_results(results,n, head=True):\n",
    "    ''' Helper function to print results\n",
    "    '''\n",
    "    if head:    \n",
    "        print('\\nTop %d from recall set of %d items:' % (n,len(results)))\n",
    "        for r in results[:n]:\n",
    "            print('\\t%0.2f - %s'%(r[0],items_t[r[1]]))\n",
    "    else:\n",
    "        print('\\nBottom %d from recall set of %d items:' % (n,len(results)))\n",
    "        for r in results[-n:]:\n",
    "            print('\\t%0.2f - %s'%(r[0],items_t[r[1]]))\n",
    "    \n",
    "\n",
    "tfm, lexicon = get_tfm(items_t)\n",
    "results = get_results_tf('led bike light', tfm , lexicon)\n",
    "print_results(results,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index\n",
    "\n",
    "This search across documents is expensive - especially if the score for many documents is zero!  To solve this problem we can create an inverted index.  An inverted index can be used to filter out documents that do not contain any of the keywords in the query before computing the ranking score.  \n",
    "\n",
    "Using our example mini-corpus:\n",
    "\n",
    "1. 'mountain bike red'\n",
    "2. 'road bike carbon'\n",
    "3. 'bike helmet'\n",
    "\n",
    "There is a toal vocabulary of six terms [ 'mountain', 'bike' , 'red', 'road', 'carbon', 'helmet' ].  An inverted index will map each of these terms to the document in which the document can be found.  \n",
    "\n",
    "| key | value |\n",
    "|-----|-------|\n",
    "| 'mountain' | [ 1 ] |  \n",
    "| 'bike' | [1, 2, 3] |  \n",
    "| 'red' | [1] |  \n",
    "| 'road' |  [2] | \n",
    "| 'carbon' | [2] | \n",
    "|  'helmet' | [3] |\n",
    "\n",
    "We could store an inverted index in a dictionary where the key is the term and the value is the document id.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "We will create an inverted index as a python dictionary keyed on the token.  \n",
    "Complete the code snippet below to create the inverted index.  Validate with the test corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(corpus):\n",
    "    idx={}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        # << POPULATE INVERTED INDEX >> CODE HERE\n",
    "        \n",
    "    return idx\n",
    "\n",
    "test_corpus = ['mountain bike red','road bike carbon','bike helmet']\n",
    "idx = create_inverted_index(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Now we can create an inverted index for all the item titles.  We can use the set intersection method to find all the documents that match the query 'led bike light'.  Run the code below checking the titles of some of the results that match query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = create_inverted_index(items_t)\n",
    "print(set(idx['led']).intersection(set(idx['bike'])).intersection(set(idx['light'])))\n",
    "print(items_t[2061])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now improve on our first ranking function.  This time only scoring the documents that match our keywords in the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_results_tf(qry, idx):\n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        for doc in idx[term]:\n",
    "            score[doc] += 1\n",
    "            \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results;\n",
    "\n",
    "\n",
    "idx = create_inverted_index(items_t)\n",
    "results = get_results_tf('led bike light', idx)\n",
    "print_results(results,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Run a few different queries some longer some shorter - for example 'front rear led light', 'led light', 'led'.  What do you notice about the ranking score?\n",
    "Try the query 'mountain bike suspension' - do the results look relevant?  What might be going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# << ENTER DIFFERENT QUERIES >> \n",
    "results = get_results_tf('mountain bike suspension', idx)\n",
    "print_results(results, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> \n",
    "The term frequency ranking is dominated by high frequency terms.  \n",
    "For example the term bike is present in nearly every other document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.plotting import output_notebook, show\n",
    "from bokeh.charts import Bar\n",
    "from bokeh.charts.attributes import CatAttr\n",
    "#from bokeh.models import ColumnDataSource\n",
    "\n",
    "df = pd.DataFrame({'term':[x for x in idx.keys()],'freq':[len(x) for x in idx.values()]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='freq',\n",
    "        plot_width=800, plot_height=400)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 3 - TF-IDF\n",
    "\n",
    "We already have all the information we need to compute IDF.  The number of documents in which a term appears is simply the length of the list of documents for a given key in our index.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Create a function definition to compute IDF.  Arguments to the function should be the term, the inverted index and the number of documents in the corpus.  The log function is in the [math](https://docs.python.org/2/library/math.html) module.  Compute the IDF of the terms 'led', 'bike' and 'light'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def idf(term, idx, n):\n",
    "    # << IMPLEMENT IDF FUNCTION >> CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rank based on TF-IDF we only need to make a few small changes to the previous TF ranking function:\n",
    "\n",
    "1. We need to know how many times the term `t` appears in `D`.  We can store this in our inverted index.  Instead of storing the document ID we can add the document ID and the number of times the term appears.  Previously this was captured in the TF matrix. We can avoid computing the TF matrix if adjust our index.\n",
    "2. We have to change the function signature in the ranking function - passing in the total size of the corpus - and we have to change the score calculation.\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>\n",
    "Modify the `get_results()` function to score documents based on TF-IDF rather than just TF.  Run a few different queries some longer some shorter - for example 'front rear led light', 'led light', 'led'.  What do you notice about the ranking score?  How do they compare to the TF ranking?  Try the query 'mountain bike suspension' - do the results look more or less relevant than TF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(corpus):\n",
    "    idx={}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc.split():\n",
    "            if word in idx:\n",
    "                if i in idx[word]:\n",
    "                    # Update document's frequency\n",
    "                    idx[word][i] += 1\n",
    "                else:\n",
    "                    # Add document\n",
    "                    idx[word][i] = 1\n",
    "            else:\n",
    "                # Add term\n",
    "                idx[word] = {i:1}\n",
    "    return idx\n",
    "\n",
    "def get_results_tfidf(qry, idx, n):\n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        # << IMPLEMENT TF-IDF SCORING >> CODE HERE\n",
    "        \n",
    "        \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            results.append([x[1],x[0]])\n",
    "    \n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results\n",
    "\n",
    "idx = create_inverted_index(items_t)\n",
    "\n",
    "print_results(results,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>We can plot the relationship between TF and IDF and get more intuition for what TF and IDF is all about.  Plotting data to understand algorithms is good practice - not only to develop intuition but also to spot bugs in your code!  We will revisit this later in the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import vplot\n",
    "\n",
    "idx = create_inverted_index(items_t)\n",
    "\n",
    "df = pd.DataFrame({'term':[x for x in idx.keys()],'freq':[len(x) for x in idx.values()],\n",
    "                  'idf':[idf(x, idx, len(items_t)) for x in idx.keys()]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p1 = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='freq',\n",
    "        plot_width=800, plot_height=400)\n",
    "p2 = Bar(df.sort_values('freq', ascending=False)[:30], label=CatAttr(columns=['term'], sort=False), values='idf',\n",
    "        plot_width=800, plot_height=400)\n",
    "p = vplot(p1, p2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problematic Queries\n",
    "\n",
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>  \n",
    "\n",
    "Although we have fixed the suspension query get another problem with the query \"mountain bikes\" which seems to just return a heap of accessories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = create_inverted_index(items_t)\n",
    "results = get_results_tfidf('mountain bike', idx, len(items_t))\n",
    "print_results(results,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to penalise items where there are many more terms in the query.  For example the terms \"mountain\" and \"bike\" only make up 2 / 12 terms in the \"oakley mens automatic mountain mtb factory lite mountain bmx bike gloves large\" yet it scores highly because there is no penalty for all the other terms in the item title.\n",
    "\n",
    "In addition this scheme create discrete levels based on combination of word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'title':[items_t[x[1]] for x in results]})\n",
    "\n",
    "d = df.groupby('score').first().reset_index()\n",
    "\n",
    "r1 = re.compile('(bike)')\n",
    "r2 = re.compile('(mountain)')\n",
    "\n",
    "for i, t in enumerate(d.title):\n",
    "    n1 = r1.findall(t)\n",
    "    n2 = r2.findall(t)\n",
    "    print('%d x Bike, %d x Mountain, Score = %0.2f'%(len(n1),len(n2),d.score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import output_notebook, show\n",
    "from bokeh.charts import Scatter\n",
    "\n",
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "\n",
    "output_notebook(hide_banner=True)\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we do not want scores to be the same for lots of documents. High TF-IDF scores in shorter documents should be more relevant - so we could try by boosting the score for documents that are shorter than average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results_tfidf_boost(qry, corpus):\n",
    "    idx = create_inverted_index(corpus)\n",
    "    n = len(corpus)\n",
    "    d = [len(x.split()) for x in corpus]\n",
    "    d_avg = float(sum(d)) / len(d)\n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idx:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                f = float(idx[term][doc])\n",
    "                score[doc] += i *  ( f / (float(d[doc]) / d_avg) )\n",
    "        \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            # output [0] score, [1] doc_id\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = get_results_tfidf_boost('mountain bike', items_t)\n",
    "print_results(results, 10)\n",
    "\n",
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "\n",
    "output_notebook()\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better!  This intuition will be built upon in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Set 4 - Implementing BM25\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'> In this exercise the goal is to implement the BM25 algorithm.   \n",
    "To help you I have provided comments breaking this down into steps 1 through 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results_bm25(qry, corpus, k1=1.5, b=0.75):\n",
    "    idx = create_inverted_index(corpus)\n",
    "    # 1.Assign (integer) n to be the length of the corpus\n",
    "    \n",
    "    # 2.Assign (list) d with elements corresponding to the length of each document in the corpus\n",
    "    \n",
    "    # 3.Assign (float) d_avg as the average document length of the documents in the corpus\n",
    "                 \n",
    "    score = Counter()\n",
    "    for term in qry.split():\n",
    "        if term in idx:\n",
    "            i = idf(term, idx, n)\n",
    "            for doc in idx[term]:\n",
    "                # 4.Assign (float) f equal to the number fo times the term appears in doc\n",
    "                \n",
    "                # 5.Assign (float) s the BM25 score for this (term, document) pair\n",
    "                \n",
    "                score[doc] += s\n",
    "                \n",
    "    results=[]\n",
    "    for x in [[r[0],r[1]] for r in zip(score.keys(), score.values())]:\n",
    "        if x[1] > 0:\n",
    "            results.append([x[1],x[0]])\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda t: t[0] * -1 )\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = get_results_bm25('mountain bike', items_t)\n",
    "print_results(results, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'> Test BM25 with different parameter values.  \n",
    "Can you observe the effect of varying k1 and b?  What happens if k1=0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = get_results_bm25('mountain bike', items_t, k1=1.5, b=0.75)\n",
    "\n",
    "# Plot score vs item length\n",
    "df = pd.DataFrame({'score':[float(x[0]) for x in results],\n",
    "                   'length':[len(items_t[x[1]].split()) for x in results]})\n",
    "output_notebook()\n",
    "p = Scatter(df, x='score', y='length')\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
