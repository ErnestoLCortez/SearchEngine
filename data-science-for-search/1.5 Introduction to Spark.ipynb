{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Spark\n",
    "\n",
    "Whenever we work in Spark the first thing we need is the spark contect (sc).  We are going to use the module `findspark` to get access to the spark context.  First we need to install the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): findspark in /home/ubuntu/anaconda2/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 8.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark\n",
    "#/home/ubuntu/workspace/spark-1.6.0-bin-hadoop2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the path to spark - which for us is on the local VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init('/home/ubuntu/workspace/spark-1.6.0-bin-hadoop2.6')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import pyspark and get the spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f4ddbeebb10>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD\n",
    "\n",
    "From the Spark documentation:\n",
    "\n",
    "_\"A Resilient Distributed Dataset (RDD), the basic abstraction in Spark, represents an immutable, partitioned collection of elements that can be operated on in parallel.\"_\n",
    "\n",
    "_\"Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\"_ \n",
    "\n",
    "For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[89] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD exists in the Spark Context which may or may not be in the notebook kernel.\n",
    "\n",
    "We apply transformations and actions to the RDD. The RDD will execute operations in parallel, for example to add up elements of list.\n",
    "\n",
    "Spark is heavily functional (built in Scala).  For example, map, reduce and filter operations are supported - these functions take functions or lambda functions as arguments: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics - transformations and actions\n",
    "\n",
    "The RDD is not loaded in memory - it is just a pointer to the file.  Spark allows us to apply transformations to the RDD - but these are computed immediately - Spark is intentionally lazy.  Nothing is computed until we execute an action, at which point the Spark driver creates tasks to run on separate nodes in the Spark cluster.  Each node executes the transformations and actions and returns the results to the driver.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "PythonRDD[168] at RDD at PythonRDD.scala:43\n"
     ]
    }
   ],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2)\n",
    "print(type(distData))\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from operator import add, mul \n",
    "\n",
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2) \\\n",
    "                .reduce(add)\n",
    "print(type(distData))\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2) \\\n",
    "                .reduce(mul)\n",
    "print(type(distData))\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Run the examples above.  Compare the `type` of object before and after the reduce action is applied.  Why does it change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "We can also create RDDs from external data sources such as Hadoop, Amazon S3 and files. Here we will create a text file RDD.  NOte that we must use absolute paths since this code is pushed onto the Spark cluster - it is not run in the context of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[98] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'2,\"ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING\"',\n",
       " u'3,\"Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm\"',\n",
       " u'4,\"BICYCLE RIMS 26\"\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE\"',\n",
       " u'5,\"Mavic Crossride 26\"\" Mountain bike wheels and WTB Weirwolf Tires\"',\n",
       " u'6,\"New KCNC ARROW 7050 Alloy Stem ',\n",
       " u'7,\"ROTOR QXL Aero Oval Road Chainring BCD110x5 53t\"',\n",
       " u'8,\"Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders\"',\n",
       " u'9,\"Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)\"',\n",
       " u'10,\"THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED\"',\n",
       " u'11,\"BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L\"']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(os.getcwd()+'/data/bike-item-titles.txt')\n",
    "print(rdd)\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "\n",
    "To illustrate RDD basics, consider the simple program below which counts the number of words in the text file rdd we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9894\n"
     ]
    }
   ],
   "source": [
    "words_per_line = rdd.map(lambda s: len(s[0].split()))\n",
    "\n",
    "total_words = words_per_line.reduce(add)\n",
    "\n",
    "\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>  To reiterate - `words_per_line` is applies a transformation to the rdd - it is not evaluated until we apply an action - such as `reduce()`.  We can inspect the transformations applied to the RDD using the `toDebugString()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[101] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[98] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  /home/ubuntu/workspace/data-science-for-search/data/bike-item-titles.txt HadoopRDD[97] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print(words_per_line.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency in Spark\n",
    "\n",
    "Many ways to do this - here are two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "terms1 = rdd.flatMap(lambda s : s.split(' ')) \\\n",
    "            .countByValue()\n",
    "\n",
    "terms2 = rdd.flatMap(lambda s : s.split()) \\\n",
    "            .map(lambda w : (w, 1)) \\\n",
    "            .reduceByKey(lambda x,y : x+y) \\\n",
    "            .collectAsMap()\n",
    "\n",
    "print(terms1['bike'])\n",
    "print(terms2['bike'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>`reduceByKey` merges the values for each key using an associative reduce function.  For example if we had a key had values [1,2,3,4] then reduce by key first computes 1+2=3, then adds the results to the next value 3+3=6, and then adds the result to the next value 6+4=10 until the list has been processed.  Associative means not dependent on the order of the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Experiment with the flatmap and map transformations.  \n",
    "What are the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toDebugString() starts to get more interesting with bigger pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[112] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[111] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[110] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[109] at reduceByKey at <ipython-input-43-3254cf225807>:1 []\n",
      "    |  PythonRDD[108] at reduceByKey at <ipython-input-43-3254cf225807>:1 []\n",
      "    |  MapPartitionsRDD[98] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |  /home/ubuntu/workspace/data-science-for-search/data/bike-item-titles.txt HadoopRDD[97] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.flatMap(lambda s : s.split()) \\\n",
    "            .map(lambda w : (w, 1)) \\\n",
    "            .reduceByKey(lambda x,y : x+y).toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames API\n",
    "\n",
    "If you look carefully above the text file is represented as a CSV and we did not parse the lines correctly.  CSV parsing is complex - but is made easier using Spark Data Frames which is an abstraction on top of RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  2|ZIPP VUKA CARBON ...|\n",
      "|  3|Cycling Bicycle M...|\n",
      "|  4|BICYCLE RIMS 26\"x...|\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "| 11|BV Bike Rear Sadd...|\n",
      "| 12|HELIX BMX ROUND D...|\n",
      "| 13|Waterproof Bicycl...|\n",
      "| 14|Brand New CycleOp...|\n",
      "| 15|Planet Bike LED S...|\n",
      "| 16|Bike Bicycle Head...|\n",
      "| 17|New Helmet Teenag...|\n",
      "| 18|2 Pcs Bike Roller...|\n",
      "| 19|FSA BICYCLE COMPR...|\n",
      "| 20|Kenda Tube 26 X1....|\n",
      "| 21|Bicycle Lock Set ...|\n",
      "| 22|NEW DT Swiss 350 ...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "        .options(header='false', inferSchema='true') \\\n",
    "        .load('/home/ubuntu/workspace/data-science-for-search/data/bike-item-titles.txt') \\\n",
    "        .selectExpr(\"C0 as id\",\"C1 as item_title\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(item_title,StringType,true)))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames API has functional model that can be applied to data frame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, item_title: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['id'] >=5).filter(df['id'] <= 10)\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).count()\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also has SQL interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'bikeitems')\n",
    "sqlContext.tableNames()\n",
    "\n",
    "sqlContext.sql(\"select id, item_title from bikeitems where id between 5 and 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert back and forth RDD <> DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, item_title=u'ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING'),\n",
       " Row(id=3, item_title=u'Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm'),\n",
       " Row(id=4, item_title=u'BICYCLE RIMS 26\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE'),\n",
       " Row(id=5, item_title=u'Mavic Crossride 26\" Mountain bike wheels and WTB Weirwolf Tires'),\n",
       " Row(id=7, item_title=u'ROTOR QXL Aero Oval Road Chainring BCD110x5 53t'),\n",
       " Row(id=8, item_title=u'Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders'),\n",
       " Row(id=9, item_title=u'Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)'),\n",
       " Row(id=10, item_title=u'THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED'),\n",
       " Row(id=11, item_title=u'BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L'),\n",
       " Row(id=12, item_title=u'HELIX BMX ROUND DROPOUT SAVERS -FITS NEARLY ALL FRAMES -Fits 3/8\" AND 10mm Axles')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "df2 = rdd.toDF()\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ZIPP', 2),\n",
       " (u'VUKA', 2),\n",
       " (u'CARBON', 2),\n",
       " (u'AERO', 2),\n",
       " (u'BASE', 2),\n",
       " (u'BAR', 2),\n",
       " (u'AND', 2),\n",
       " (u'EXTENSIONS', 2),\n",
       " (u'COMPLETE', 2),\n",
       " (u'TRIATHLON', 2)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ ( word, row[0]) for word in row[1].split(' ') ] ) \n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', <pyspark.resultiterable.ResultIterable at 0x7f4ddb27c1d0>),\n",
       " (u'Powerlock-New', <pyspark.resultiterable.ResultIterable at 0x7f4ddab35190>),\n",
       " (u'SporstWear', <pyspark.resultiterable.ResultIterable at 0x7f4ddab35210>),\n",
       " (u'S-5', <pyspark.resultiterable.ResultIterable at 0x7f4ddab35150>),\n",
       " (u'Interloc', <pyspark.resultiterable.ResultIterable at 0x7f4ddab35050>),\n",
       " (u'yellow', <pyspark.resultiterable.ResultIterable at 0x7f4ddab351d0>),\n",
       " (u'Controltech', <pyspark.resultiterable.ResultIterable at 0x7f4ddb2b5210>),\n",
       " (u'Umbrella', <pyspark.resultiterable.ResultIterable at 0x7f4ddb2b5590>),\n",
       " (u'Adjusting', <pyspark.resultiterable.ResultIterable at 0x7f4ddb2b51d0>),\n",
       " (u'Kium', <pyspark.resultiterable.ResultIterable at 0x7f4ddb439110>)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey()\n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Unicycle', [2138, 3748, 7232, 8777])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1])))\n",
    "index.filter(lambda x : x[0] == 'Unicycle').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Unicycle', [2138, 3748, 7232, 8777])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.filter(lambda x : x[0] == 'Unicycle').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>The index has upper and lower case tokens, for example 'Unicycle' and 'unicycle'.  \n",
    "Can you modify the index to normalise the tokens to lowercase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very quick overview - however you are in a great spot to now try out more of the great examples from the [Spark documentation](http://spark.apache.org/docs/latest/)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
