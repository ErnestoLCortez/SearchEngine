{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Machine Learning for Ranking\n",
    "\n",
    "First we need to load our training data into pandas dataframe.  The data is in tab separated formatand the easiest way to load this is using the ```pandas``` ```read_table()``` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table(\"data/fullDataset.tsv\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the shape and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79575, 19)\n",
      "Index([u'key', u'query', u'Title', u'LeafCats', u'ItemID', u'X_unit_id',\n",
      "       u'SCORE', u'label_relevanceGrade', u'label_relevanceBinary',\n",
      "       u'feature_1', u'feature_2', u'feature_3', u'feature_4', u'feature_5',\n",
      "       u'feature_6', u'feature_7', u'feature_8', u'feature_9', u'feature_10'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>query</th>\n",
       "      <th>Title</th>\n",
       "      <th>LeafCats</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>X_unit_id</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>label_relevanceGrade</th>\n",
       "      <th>label_relevanceBinary</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25297</th>\n",
       "      <td>40418</td>\n",
       "      <td>argand</td>\n",
       "      <td>BRASS \"FLAT TOP\" DEFLECTOR/CHIMNEY RING FOR AR...</td>\n",
       "      <td>1407</td>\n",
       "      <td>20244</td>\n",
       "      <td>740191598</td>\n",
       "      <td>3:-1:3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3093.916748</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.622210</td>\n",
       "      <td>-4.739789</td>\n",
       "      <td>46.223152</td>\n",
       "      <td>0.388945</td>\n",
       "      <td>38</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>219</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>15736</td>\n",
       "      <td>age of empires 3</td>\n",
       "      <td>Age of Empires III: Gold Edition    WIN XP    ...</td>\n",
       "      <td>139973</td>\n",
       "      <td>8060</td>\n",
       "      <td>724905631</td>\n",
       "      <td>3:3:3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2952.142822</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.121331</td>\n",
       "      <td>-5.078777</td>\n",
       "      <td>53.495548</td>\n",
       "      <td>0.674927</td>\n",
       "      <td>93</td>\n",
       "      <td>333333</td>\n",
       "      <td>143</td>\n",
       "      <td>1.607907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>78514</td>\n",
       "      <td>alton ellis</td>\n",
       "      <td>ALTON ELLIS Sunday Coming/Another Night STUDIO...</td>\n",
       "      <td>176985</td>\n",
       "      <td>39005</td>\n",
       "      <td>740197728</td>\n",
       "      <td>3:3:3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2873.857178</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.882324</td>\n",
       "      <td>-4.961157</td>\n",
       "      <td>25.345171</td>\n",
       "      <td>0.489932</td>\n",
       "      <td>151</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>172</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72072</th>\n",
       "      <td>19626</td>\n",
       "      <td>woman shoes wedges</td>\n",
       "      <td>Womens Fashion Sandals Cute Wedge Heel Sandal ...</td>\n",
       "      <td>55793|62107</td>\n",
       "      <td>9937</td>\n",
       "      <td>724908777</td>\n",
       "      <td>3:3:3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3560.105225</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.081261</td>\n",
       "      <td>-4.514035</td>\n",
       "      <td>69.695389</td>\n",
       "      <td>0.508286</td>\n",
       "      <td>440205</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>225</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57503</th>\n",
       "      <td>23660</td>\n",
       "      <td>amc cars</td>\n",
       "      <td>AMC PACER AUTO ICONS STRIP OF 10 MINT CAR STAM...</td>\n",
       "      <td>14024|1313</td>\n",
       "      <td>11897</td>\n",
       "      <td>753410348</td>\n",
       "      <td>-1:-1:-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>760.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.390667</td>\n",
       "      <td>-4.736623</td>\n",
       "      <td>18.733725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1335</td>\n",
       "      <td>-1000000</td>\n",
       "      <td>139</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         key               query  \\\n",
       "25297  40418              argand   \n",
       "8724   15736    age of empires 3   \n",
       "1562   78514         alton ellis   \n",
       "72072  19626  woman shoes wedges   \n",
       "57503  23660            amc cars   \n",
       "\n",
       "                                                   Title     LeafCats  ItemID  \\\n",
       "25297  BRASS \"FLAT TOP\" DEFLECTOR/CHIMNEY RING FOR AR...         1407   20244   \n",
       "8724   Age of Empires III: Gold Edition    WIN XP    ...       139973    8060   \n",
       "1562   ALTON ELLIS Sunday Coming/Another Night STUDIO...       176985   39005   \n",
       "72072  Womens Fashion Sandals Cute Wedge Heel Sandal ...  55793|62107    9937   \n",
       "57503  AMC PACER AUTO ICONS STRIP OF 10 MINT CAR STAM...   14024|1313   11897   \n",
       "\n",
       "       X_unit_id     SCORE  label_relevanceGrade  label_relevanceBinary  \\\n",
       "25297  740191598    3:-1:3                     5                      0   \n",
       "8724   724905631     3:3:3                     6                      1   \n",
       "1562   740197728     3:3:3                     6                      1   \n",
       "72072  724908777     3:3:3                     6                      1   \n",
       "57503  753410348  -1:-1:-1                     2                      0   \n",
       "\n",
       "         feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "25297  3093.916748          1  -7.622210  -4.739789  46.223152   0.388945   \n",
       "8724   2952.142822          1  -8.121331  -5.078777  53.495548   0.674927   \n",
       "1562   2873.857178          1  -7.882324  -4.961157  25.345171   0.489932   \n",
       "72072  3560.105225          1  -9.081261  -4.514035  69.695389   0.508286   \n",
       "57503   760.000000          1  -7.390667  -4.736623  18.733725   0.000000   \n",
       "\n",
       "       feature_7  feature_8  feature_9  feature_10  \n",
       "25297         38   -1000000        219 -100.000000  \n",
       "8724          93     333333        143    1.607907  \n",
       "1562         151   -1000000        172 -100.000000  \n",
       "72072     440205   -1000000        225 -100.000000  \n",
       "57503       1335   -1000000        139 -100.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "\n",
    "Column name             | Description\n",
    "------------------------|-----------------------------------------------------------------------\n",
    "key                     |  Used to join back to the original dataset and add any additional fields as needed\n",
    "query                   |  Un-normalized query keywords (without user constraints)\n",
    "Title                   |  Un-normalized title\n",
    "LeafCats                |  Item leaf category IDs\n",
    "ItemID                  |  Anonymized itemID. This is not the actual item ID\n",
    "X_unit_id               |  The query ID used for grouping query-item pairs by their search, primarily for per-query metrics. Essentially it's a \"search ID\". We can also group by query or normalized query\n",
    "SCORE                   |  The scores given by up to three judges characterizing the relevance/relevance problem of the query-item pair\n",
    "label_relevanceGrade    |  The SCORE averaged and rounded and converted to a relevance graded judgment 0-6, 6 being the best. Note this is very approximate\n",
    "label_relevanceBinary   |  The SCORE converted to a binary relevant(==1) or not relevant (==0) judgment.  This is a more accurate label than the Grade, I recommend it as a training target\n",
    "\n",
    "Features (In brief): \n",
    "\n",
    "* query features: feature_2, feature_7, feature_8\n",
    "* item features: feature_3, feature_4, feature_9\n",
    "* query-item features: feature_1, feature_5, feature_6, feature_10\n",
    "\n",
    "## Getting ready for Machine Learning\n",
    "\n",
    "We start by simply exploring how we might classify queries as relevent / not relevant.  We will explore a series of different models to do this. The first is logistic regression, we will also use SVM and finally classification tree's.  Along the way we will look at over fitting / generalization and how to evaluate models.  \n",
    "\n",
    "We can do all this using Spark MLLIB - first we have to findspark and get our spark context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(os.getenv('HOME') + '/spark-1.6.0-bin-hadoop2.6')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f17779a0a50>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to load the tsv data into a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(key,IntegerType,true),StructField(query,StringType,true),StructField(Title,StringType,true),StructField(LeafCats,StringType,true),StructField(ItemID,IntegerType,true),StructField(X_unit_id,IntegerType,true),StructField(SCORE,StringType,true),StructField(label_relevanceGrade,IntegerType,true),StructField(label_relevanceBinary,IntegerType,true),StructField(feature_1,DoubleType,true),StructField(feature_2,IntegerType,true),StructField(feature_3,DoubleType,true),StructField(feature_4,DoubleType,true),StructField(feature_5,DoubleType,true),StructField(feature_6,DoubleType,true),StructField(feature_7,IntegerType,true),StructField(feature_8,DoubleType,true),StructField(feature_9,IntegerType,true),StructField(feature_10,DoubleType,true)))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import os\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options() \\\n",
    "        .options(header='true', inferSchema='true', delimiter='\\t') \\\n",
    "        .load(os.getcwd() + '/data/fullDataset.tsv') \n",
    "        \n",
    "df.schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the features and the target for the machine learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'dataset')\n",
    "sqlContext.tableNames()\n",
    "\n",
    "data_full = sqlContext.sql(\"select label_relevanceBinary, feature_1, feature_2, feature_3, feature_4 \\\n",
    "                       feature_5, feature_6, feature_7, feature_8, feature_9, feature_10 \\\n",
    "               from dataset\").rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the data into test and validation data sets - splitting 75%:25% between the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parseRecord(line):\n",
    "    return LabeledPoint(line[0], line[1:])\n",
    "\n",
    "data_train, data_test = data_full.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data records = 59892\n",
      "Training data records = 19559\n"
     ]
    }
   ],
   "source": [
    "print('Training data records = ' + str(data_train.count()))\n",
    "print('Training data records = ' + str(data_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an SVM - a simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SVMWithSGD.train(data_train.map(parseRecord), iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights=[1194.26941609,0.461463715096,-2.21656945582,-0.993141240628,0.2438952531,24884.355726,63515.4566756,51.6824038304,-11.1044683869], intercept=0.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.489422642078\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "preds = data_test.map(parseRecord).map(lambda p: (p.label, model.predict(p.features)))\n",
    "err = preds.filter(lambda (v, p): v != p).count() / float(data_test.count())\n",
    "print(\"Training Error = \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(data_train.map(parseRecord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights=[0.00042945137487,0.639302892683,0.0172727048381,0.235558307927,1.13591233639,3.55644013574e-07,-1.32957685793e-07,1.88841414344e-05,0.000132492242944], intercept=0.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.290688910105\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "preds = data_test.map(parseRecord).map(lambda p: (p.label, model.predict(p.features)))\n",
    "err = preds.filter(lambda (v, p): v != p).count() / float(data_test.count())\n",
    "print(\"Training Error = \" + str(err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
