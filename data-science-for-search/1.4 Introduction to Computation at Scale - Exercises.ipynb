{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.4 Introduction to Computation at Scale\n",
    "\n",
    "We are going to use the python [mrjob](https://github.com/Yelp/mrjob) package developed at Yelp.\n",
    "\n",
    "This package allows us to develop and test map reduce jobs locally and when ready deploy them to a hadoop cluster with hadoop streaming enabled.  We are going to use it to run jobs locally.\n",
    "\n",
    "To write a map reduce job we need to implement mapper() and reducer() functions.  The mrjob package takes care of the orchestration of the job.  Here is a first example that will count words in a file.  \n",
    "\n",
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>To edit the file we are using the Jupyter Notebook Cell Magic '%%file'.  \n",
    "The file is written to the file system by the notebook when the cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcounter.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcounter.py \n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        yield \"words\", len(value.split())\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key points to note:\n",
    "\n",
    "* We inherit from the class MRJob and provide at least one mapper, reducer or combiner method implementation\n",
    "* All python methods take `self` as their first argument - this is normal - not mrjob specific\n",
    "* The mappers will be sent a partition of the input data\n",
    "* The mappers must yield a key value pair - the emitted key value pairs will be sent to reducers - hash function maps the key uniquely to a node\n",
    "* The mappers and reducers are implemented as Python [generators](https://wiki.python.org/moin/Generators) - allowing the function to be used like an iterator\n",
    "* The reducers will receive the key and all the values emitted by the mappers with this key\n",
    "* The reducers must also output key and value pairs\n",
    " \n",
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>The job is scheduled form the command line.  \n",
    "We can access the shell with the Jupyter Notebook line magic '!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python wordcounter.py data/bike-item-titles-clean.txt > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process runs and the output is dumped into the file out.txt.  In this case there is just a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cat out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have one pass through the file and have computed just the number of words.  We can have more elaborate jobs that compute multiple statistics.  Here we count characters, word and line count - the mapper emits three key value pairs for each line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file wordcounter.py \n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        yield \"chars\", len(value)\n",
    "        yield \"words\", len(value.split())\n",
    "        yield \"lines\", 1\n",
    "        \n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python wordcounter.py data/bike-item-titles.txt > out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! cat out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency in Map Reduce\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Using the word count example above can you modify the MR job to compute token frequency across the entire corpus in file `data/bike-item-titles.txt`?  Remember you can only emit (key, value) pairs from the mapper.\n",
    "\n",
    "\n",
    "**Hint** : the `/data/bike-item-titles.txt` file is quoted like a CSV file.  The easiest way to handle the CSV input presented to the mapper is to use StringIO and csv.reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "import csv\n",
    "\n",
    "line = '\"Some quoted text about 18\"\" pizzas\"'\n",
    "for row in csv.reader(StringIO.StringIO(line)):\n",
    "    print(row)\n",
    "    for term in row[0].split():\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file term-frequency.py \n",
    "from mrjob.job import MRJob\n",
    "import StringIO\n",
    "import csv\n",
    "\n",
    "class MRTermFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        # << IMPLEMENT MAPPER >> CODE HERE\n",
    "        \n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # << IMPLEMENT REDUCER >> CODE HERE\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTermFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python term-frequency.py data/bike-item-titles.txt > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Using a line magic `grep` the output file for the term bike.  \n",
    "You may want to pipe the results of `grep` to `head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! grep 'bike' out.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index\n",
    "\n",
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'> The goal is to creat an inverted index mapping terms to rows in the file using MRJob.  The row id is in the first column of the file.  \n",
    "The input file should be `data/bike-item-titles.txt`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file inverted-index.py \n",
    "from mrjob.job import MRJob\n",
    "import StringIO\n",
    "import csv\n",
    "\n",
    "class MRInvertedIndex(MRJob):\n",
    "\n",
    "    def mapper(self, _, value):\n",
    "        # << IMPLEMENT MAPPER >> CODE HERE\n",
    "        \n",
    "                    \n",
    "    def reducer(self, key, values):\n",
    "        # << IMPLEMENT MAPPER >> CODE HERE\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRInvertedIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python inverted-index.py data/bike-item-titles.txt > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>`grep` the output file to find the row numbers where the item title includes the term 'unicycle'.  \n",
    "Use the UNIX command `awk`, or other UNIX command of your liking, to extract one of those lines to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# << GREP AND AWK >> CODE HERE\n",
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
